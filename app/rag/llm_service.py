# services/chat_service.py
import json
from typing import AsyncGenerator
from app.db.mongo import get_mongo
from app.models.conversation import UserMessage
from openai import AsyncOpenAI

from app.rag.mesage import find_depth_parent_mesage
from app.core.logging import logger
from app.db.milvus import milvus_client
from app.rag.get_embedding import get_embeddings_from_httpx
from app.rag.utils import replace_image_content, sort_and_filter

# LiteLLM ç›¸å…³å¯¼å…¥
try:
    from litellm import acompletion
    LITELLM_AVAILABLE = True
except ImportError:
    LITELLM_AVAILABLE = False
    acompletion = None


class ChatService:

    @staticmethod
    async def create_chat_stream(
        user_message_content: UserMessage, message_id: str
    ) -> AsyncGenerator[str, None]:
        """åˆ›å»ºèŠå¤©æµå¹¶å¤„ç†å­˜å‚¨é€»è¾‘"""
        db = await get_mongo()

        # è·å–system prompt
        model_config = await db.get_conversation_model_config(
            user_message_content.conversation_id
        )

        model_name = model_config["model_name"]
        model_url = model_config["model_url"] 
        api_key = model_config["api_key"]     
        base_used = model_config["base_used"]
        provider_type = model_config.get("provider_type", "qwen")

        print(f"Provider: {provider_type}, Model: {model_name}")

        # ç»Ÿä¸€ä½¿ç”¨ LiteLLM å¤„ç†æ‰€æœ‰æ¨¡å‹
        if not LITELLM_AVAILABLE:
            raise Exception("LiteLLM is required but not available. Please install with: pip install litellm")
        
        print(f"ğŸš€ Using unified LiteLLM architecture for all models")

        system_prompt = model_config["system_prompt"]
        if len(system_prompt) > 1048576:
            system_prompt = system_prompt[0:1048576]

        temperature = model_config["temperature"]
        if temperature < 0 and not temperature == -1:
            temperature = 0
        elif temperature > 1:
            temperature = 1
        else:
            pass

        max_length = model_config["max_length"]
        if max_length < 1024 and not max_length == -1:
            max_length = 1024
        elif max_length > 1048576:
            max_length = 1048576
        else:
            pass

        top_P = model_config["top_P"]
        if top_P < 0 and not top_P == -1:
            top_P = 0
        elif top_P > 1:
            top_P = 1
        else:
            pass

        top_K = model_config["top_K"]
        if top_K == -1:
            top_K = 3
        elif top_K < 1:
            top_K = 1
        elif top_K > 30:
            top_K = 30
        else:
            pass

        if not system_prompt:
            system_prompt = "All outputs in Markdown format, especially mathematical formulas in Latex format($formula$)."

        # ç»„åˆå†…ç½®æç¤ºè¯å’Œè‡ªå®šä¹‰æç¤ºè¯
        built_in_prompt = """ä½ æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€AIåŠ©æ‰‹ï¼Œå¯ä»¥åŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾ç‰‡ä¿¡æ¯ã€‚å½“ç”¨æˆ·æé—®æ—¶ï¼šé‡ç‚¹å…³æ³¨å›¾ç‰‡ä¸­çš„å…³é”®ä¿¡æ¯ï¼šæ–‡å­—ã€å›¾è¡¨ã€è¡¨æ ¼ã€æ•°æ®ç­‰
**åˆ†æè¦æ±‚**ï¼š
1. ä»”ç»†åˆ†æé—®é¢˜ä¸­æåˆ°çš„æ‰€æœ‰å†…å®¹ï¼Œç»“åˆå›¾ç‰‡å†…å®¹ä¿¡æ¯è¿›è¡Œå›ç­”ï¼Œä¸è¦è”æƒ³
2. å½“ä¸ç¡®å®šæ—¶ï¼Œæ˜ç¡®è¡¨ç¤ºæ— æ³•ç¡®å®šï¼Œå¹¶è¯´æ˜éœ€è¦å“ªäº›é¢å¤–ä¿¡æ¯"""
        combined_system_prompt = f"{built_in_prompt}\n\n{system_prompt}"

        logger.info(
            f"chat '{user_message_content.conversation_id} uses system prompt {combined_system_prompt}'"
        )

        messages = [
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": combined_system_prompt,
                    }
                ],
            }
        ]

        history_messages = await find_depth_parent_mesage(
            user_message_content.conversation_id,
            user_message_content.parent_id,
            MAX_PARENT_DEPTH=5,
        )

        for i in range(len(history_messages), 0, -1):
            messages.append(history_messages[i - 1])

        # å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶
        content = []
        bases = []
        if user_message_content.temp_db:
            bases.append({"baseId": user_message_content.temp_db})

        # æœç´¢çŸ¥è¯†åº“åŒ¹é…å†…å®¹
        bases.extend(base_used)
        file_used = []
        if bases:
            result_score = []
            query_embedding = await get_embeddings_from_httpx(
                [user_message_content.user_message], endpoint="embed_text"
            )
            for base in bases:
                collection_name = f"colqwen{base['baseId'].replace('-', '_')}"
                if milvus_client.check_collection(collection_name):
                    scores = milvus_client.search(
                        collection_name, data=query_embedding[0], topk=top_K
                    )
                    result_score.extend(scores)
            sorted_score = sort_and_filter(result_score, min_score=10)
            if len(sorted_score) >= top_K:
                cut_score = sorted_score[:top_K]
            else:
                cut_score = sorted_score

            # è·å–minio nameå¹¶è½¬æˆURLï¼ŒæŒ‰åˆ†æ•°æ’åº
            for idx, score in enumerate(cut_score):
                file_and_image_info = await db.get_file_and_image_info(
                    score["file_id"], score["image_id"]
                )
                
                # æ ¹æ®æ’åæ·»åŠ ä¼˜å…ˆçº§æ ‡è¯†
                file_used.append(
                    {
                        "score": score["score"],
                        "knowledge_db_id": file_and_image_info["knowledge_db_id"],
                        "file_name": file_and_image_info["file_name"],
                        "image_url": file_and_image_info["image_minio_url"],
                        "file_url": file_and_image_info["file_minio_url"],
                    }
                )
                
                content.append(
                    {
                        "type": "image_url",
                        "image_url": file_and_image_info["image_minio_filename"],
                    }
                )

                print(file_and_image_info["image_minio_url"]) 

        # ç”¨æˆ·è¾“å…¥
        content.append(
            {
                "type": "text",
                "text": user_message_content.user_message,
            },
        )

        user_message = {
            "role": "user",
            "content": content
        }
        messages.append(user_message)
        send_messages = await replace_image_content(messages)
        
        # å‘é€ file_used ä¿¡æ¯
        file_used_payload = json.dumps(
            {
                "type": "file_used",
                "data": file_used,  # è¿™é‡Œç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„ file_used åˆ—è¡¨
                "message_id": message_id,
                "model_name": model_name,
            }
        )
        yield f"data: {file_used_payload}\n\n"

        # æ ¹æ®æ¨¡å‹æ”¯æŒæƒ…å†µé€‰æ‹©è°ƒç”¨æ–¹å¼
        full_response = []
        total_token = 0
        completion_tokens = 0
        prompt_tokens = 0
        
        try:
            if LITELLM_AVAILABLE:
                print("ğŸš€ Using LiteLLM for unified API call...")
                
                # æ ¹æ® provider_type è®¾ç½®æ¨¡å‹åç§°æ ¼å¼
                litellm_model = model_name
                if provider_type == "ollama":
                    litellm_model = f"ollama/{model_name}"
                else:
                    litellm_model = f"openai/{model_name}"
            
                # æ„å»º LiteLLM å‚æ•°
                litellm_params = {
                    "model": litellm_model,
                    "messages": send_messages,
                    "stream": True,
                }
                
                # æ·»åŠ å¯é€‰å‚æ•°
                if temperature != -1:
                    litellm_params["temperature"] = temperature
                if max_length != -1:
                    litellm_params["max_tokens"] = max_length
                if top_P != -1:
                    litellm_params["top_p"] = top_P
                    
                litellm_params["stream_options"] = {"include_usage": True}
                # è®¾ç½® API é…ç½®
                litellm_params["api_base"] = model_url
                litellm_params["api_key"] = api_key
                litellm_params["timeout"] = 60
                print(f"LiteLLM params: model={litellm_model}, messages={len(send_messages)}")
                print(f"Message content types: {[type(msg.get('content')) for msg in send_messages]}")

                response = await acompletion(**litellm_params)
                
                # å¤„ç† LiteLLM æµå“åº”
                async for chunk in response:
                    if hasattr(chunk, 'choices') and chunk.choices:
                        delta = chunk.choices[0].delta

                        if (hasattr(delta, "reasoning_content") and delta.reasoning_content != None):
                            # ç”¨JSONå°è£…å†…å®¹ï¼Œè‡ªåŠ¨å¤„ç†æ¢è¡Œç¬¦ç­‰ç‰¹æ®Šå­—ç¬¦
                            payload = json.dumps(
                                {
                                    "type": "thinking",
                                    "data": delta.reasoning_content,
                                    "message_id": message_id,
                                }
                            )
                            yield f"data: {payload}\n\n"

                        if hasattr(delta, 'content') and delta.content:
                            content_chunk = delta.content
                            payload = json.dumps(
                                {"type": "text", "data": content_chunk, "message_id": message_id}
                            )
                            full_response.append(content_chunk)
                            yield f"data: {payload}\n\n"
                            
                    # å¤„ç† token ç»Ÿè®¡
                    if hasattr(chunk, 'usage') and chunk.usage:
                        usage = chunk.usage
                        total_token = getattr(usage, 'total_tokens', 0)
                        completion_tokens = getattr(usage, 'completion_tokens', 0)
                        prompt_tokens = getattr(usage, 'prompt_tokens', 0)
                        
                        payload = json.dumps(
                            {
                                "type": "token",
                                "total_token": total_token,
                                "completion_tokens": completion_tokens,
                                "prompt_tokens": prompt_tokens,
                                "message_id": message_id,
                            }
                        )
                        yield f"data: {payload}\n\n"
                        
        except Exception as e:
            error_msg = f"API call error (LiteLLM): {str(e)}"
            print(f"Error: {error_msg}")
            print(f"Exception details: {type(e).__name__}: {str(e)}")
            
            yield f"data: {json.dumps({'type': 'error', 'data': error_msg})}\n\n"

        ai_message = {"role": "assistant", "content": "".join(full_response)}
        # ä¿å­˜AIå“åº”åˆ°mongodb
        # await repository.save_ai_message(conversation_id, "".join(full_response))

        await db.add_turn(
            conversation_id=user_message_content.conversation_id,
            message_id=message_id,
            parent_message_id=user_message_content.parent_id,
            user_message=user_message,
            temp_db=user_message_content.temp_db,
            ai_message=ai_message,
            file_used=file_used,
            status="",
            total_token=total_token,
            completion_tokens=completion_tokens,
            prompt_tokens=prompt_tokens,
        )
